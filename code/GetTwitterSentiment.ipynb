{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGJEqrHvIrpA+rR+YemJrp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FredLongo/DataMiningProject/blob/main/code/GetTwitterSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first part is about processing the Twitter data to a format we can work with.   \n"
      ],
      "metadata": {
        "id": "-gt85Llw_0cp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBQdtyBwzosZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#with open(\"twiterscraper.json\", \"r\") as f:\n",
        "\n",
        "#infile = \"/content/dataset_easy-twitter-search-scraper_2023-11-21_01-10-14-456.json\" # This has Stock\n",
        "#infile = \"/content/dataset_easy-twitter-search-scraper_2023-11-21_18-49-26-401.json\"  # username filters\n",
        "infile = '/content/dataset_easy-twitter-search-scraper_2023-11-22_01-36-29-112.json' # no username filter\n",
        "\n",
        "# Data pulled from https://console.apify.com/actors/2s3kSMq7tpuC3bI6M/runs/cMp5HramffKLAmVSt#output\n",
        "\n",
        "with open(infile, \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "LM__sE5Z0D24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Set up Username Counts\n",
        "'''\n",
        "\n",
        "username =[\"@USNewsMoney\", \"@TMFStockAdvisor\", \"@ftfinancenews\", \"@Stocktwits\", \"@MadMoneyOnCNBC\", \"@SquawkCNBC\"]\n",
        "\n",
        "cols = 2\n",
        "rows = len(username)\n",
        "username_counts = [[0 for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "# Fill the first column with values from the list\n",
        "for i in range(rows):\n",
        "    username_counts[i][0] = username[i]\n",
        "\n",
        "# Printing the 2D array\n",
        "#for row in username_counts:    print(row)\n"
      ],
      "metadata": {
        "id": "K3p57jJBdvCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Set up selectQ Counts\n",
        "'''\n",
        "searchQuery = [\"stock\",\"Apple\",\"Microsoft\",\"Alphabet\",\"Amazon\",\"Nvidia\",\"Tesla\",\"GOOG\",\"AMZN\",\"AAPL\",\"META\",\"MSFT\",\"NVDA\",\"TSLA\"]\n",
        "\n",
        "cols = 2\n",
        "rows = len(searchQuery)\n",
        "searchQuery_counts = [[0 for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "# Fill the first column with values from the list\n",
        "for i in range(rows):\n",
        "    searchQuery_counts[i][0] = searchQuery[i]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sd2MNjT4zDOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point I want to add the code to do the sentiment alalisis.\n",
        "\n"
      ],
      "metadata": {
        "id": "n-fnSw-Im217"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "\n",
        "\n",
        "# load model and tokenizer\n",
        "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
        "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
        "\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n"
      ],
      "metadata": {
        "id": "75BicuOmm9rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocesses_tweet(tweet):\n",
        "  tweet_words = []\n",
        "\n",
        "  # replace\n",
        "  for word in tweet.split(' '):\n",
        "    # username\n",
        "    if word.startswith('@') and len(word) > 1 :\n",
        "      word = '@user'\n",
        "\n",
        "    # http link\n",
        "    elif word.startswith('http'):\n",
        "      word = \"http\"\n",
        "\n",
        "    elif word == '':\n",
        "      continue\n",
        "\n",
        "    tweet_words.append(word)\n",
        "\n",
        "  tweet_proc = \" \".join(tweet_words)\n",
        "  return tweet_proc\n"
      ],
      "metadata": {
        "id": "7dWCHf6CrFur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweet_sentiment(tweet_proc):\n",
        "  # sentiment analysis\n",
        "  encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')\n",
        "  # output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])\n",
        "  output = model(**encoded_tweet)\n",
        "\n",
        "  scores = output[0][0].detach().numpy()\n",
        "  scores = softmax(scores)\n",
        "\n",
        "  maxposition = 0\n",
        "  maxscore = 0\n",
        "\n",
        "  for i in range(len(scores)):\n",
        "    if maxscore <= scores[i]:\n",
        "      maxposition = i\n",
        "      maxscore = scores[i]\n",
        "\n",
        "  return labels[maxposition]  , scores[maxposition]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kofDI5AUtuy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_data = []\n",
        "loop_count = -1\n",
        "\n",
        "# Need to create a tweet loop\n",
        "\n",
        "for x in data:\n",
        "  loop_count+=1\n",
        "  try:\n",
        "    tweet_proc = preprocesses_tweet(x['text'])\n",
        "    sentiment_label, sentiment_score = get_tweet_sentiment(tweet_proc)\n",
        "  except Exception as e:\n",
        "    print(f\"exception on {str(loop_count)}\")\n",
        "    print(f\"Text:\",x['text'])\n",
        "    continue\n",
        "\n",
        "  out_data.append(\n",
        "        {\"timestamp\":x['timestamp']\n",
        "        ,\"username\":x['user']['username']\n",
        "        ,\"searchQuery\":x['searchQuery']\n",
        "        ,\"sentement\":sentiment_label\n",
        "        ,\"score\":str(sentiment_score)\n",
        "        ,\"text\":tweet_proc})\n",
        "\n"
      ],
      "metadata": {
        "id": "aWoAqEnpm-r0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547de248-2f3d-4a1b-8e6b-c1a9d1176e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exception on 66\n",
            "exception on 94\n",
            "exception on 222\n",
            "exception on 299\n",
            "exception on 412\n",
            "exception on 557\n",
            "exception on 625\n",
            "exception on 743\n",
            "exception on 856\n",
            "exception on 907\n",
            "exception on 948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_name = infile[:-5] + \"_out.csv\"\n",
        "\n",
        "# Open the CSV file in write mode\n",
        "with open(csv_file_name, mode='w', newline='') as csv_file:\n",
        "    # Create a CSV writer\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write the header row based on the keys in the JSON data\n",
        "    header = out_data[0].keys()\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write each row of data from the JSON object\n",
        "    for row in out_data:\n",
        "        csv_writer.writerow(row.values())"
      ],
      "metadata": {
        "id": "lohpJGAryXdK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}