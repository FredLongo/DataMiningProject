{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPAZ7auhuTH/4tSVWynofU9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FredLongo/DataMiningProject/blob/main/code/GetTwitterSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first part is about processing the Twitter data to a format we can work with.   \n"
      ],
      "metadata": {
        "id": "-gt85Llw_0cp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Overview\n",
        "\n",
        "## Evaluation of Twiter stock sentement on price stock prices.\n",
        "\n",
        "In this project we evaluate twiter tweet posts to see if tweets can act as leading indecators or laging indicaterst to stock prices.  We do this over a one day, three day, five day pre and post of tweet.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mEwxVGti9a79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aquiring Tweets\n",
        "\n",
        "We targeted twitter tweets to data mine as a source for evaluation of sentament.  Twitter was chosen because many of the paper we reviewed took this approch.  We wanted to emulate some of their process.  However Twitter has changed it's policies over the past year to not allow twitter pulls for free to research.  As a result We needed to use a thired party product to pull a few months worth of tweets.  This third party was Apify (www.apify.com)  as a tool it can pull twiter feeds.\n",
        "\n",
        "\n",
        "## Limiting Data set\n",
        "During this process we thought about the diffrent approches to evaluation of stock.  We could look at all tweets and get sentenment with respect to the market as a whole.  However we found that their are so many ways to iterprete the tweets with respect to the market as a whole was difficalt.  We chose to limit our stocks to just the top 7 of the S&P 500.  By doing so we felt this this would be easerir to selectivly identify tweets with respect to the stock.  The stocks we chose are \"Apple\",\"Alphabet\",\"Amazon\",\"Microsoft\",\"Meta\",\"Nvidia\" and \"Tesla\".  Using their Stock lable of \"AAPL\",\"GOOG\",\"AMZN\",\"MSFT\",\"META\",\"NVDA\",\"TSLA\"\n",
        "\n",
        "We also just looked a a few months of tweets.  So as to limit the data set to just a sample timeframe.  \n",
        "\n",
        "Twitter has many users(channels) that produce tweets. We recognize not all channels are intresed in producing relevent, relyable and consistent statements with respect to market or stocks.  So we limit the channals that we monitored to just a few key player that we though might be relevent in the marketspace of stock evaluations.  The channals we chose to evaluate are \"@USNewsMoney\", \"@TMFStockAdvisor\", \"@ftfinancenews\", \"@Stocktwits\", \"@MadMoneyOnCNBC\" and \"@SquawkCNBC\".  We found these to be popular that should fit our needs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KuQ7215q_4uA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading tweet data."
      ],
      "metadata": {
        "id": "LEz7VDQ1FIVJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SBQdtyBwzosZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "#This pulls in the Tweets data\n",
        "#Tweets URL of the file to be downloaded\n",
        "\n",
        "Tweets_url = 'https://drive.google.com/file/d/1s429__b-2Gptf2oN2g-t--1aNPkMiyU-/view?usp=drive_link'\n",
        "\n",
        "Tweets_filename = 'Tweets.json'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "Tweets_response = requests.get(Tweets_url)\n",
        "\n",
        "# Ensure the request was successful\n",
        "if Tweets_response.status_code == 200:\n",
        "    # Write the content of the response to a file\n",
        "    with open(Tweets_filename, 'wb') as file:\n",
        "        file.write(Tweets_response.content)\n",
        "else:\n",
        "    print(f\"Failed to download the file. Status code: {Tweets_response.status_code}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7dVV8pfAKiki"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Stock Price URL of the file to be downloaded\n",
        "StockPrice_url = 'https://drive.google.com/file/d/1Hk3Ca66ai_vAO14EFAD1AfOEk2A60E-U/view?usp=drive_link'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "StockPrice_response = requests.get(StockPrice_url)\n",
        "\n",
        "# Ensure the request was successful\n",
        "if StockPrice_response.status_code == 200:\n",
        "    # Write the content of the response to a file\n",
        "    with open('StockPrice.csv', 'wb') as file:\n",
        "        file.write(StockPrice_response.content)\n",
        "else:\n",
        "    print(f\"Failed to download the file. Status code: {StockPrice_response.status_code}\")\n"
      ],
      "metadata": {
        "id": "kUpZkFsVrO7h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#with open(\"twiterscraper.json\", \"r\") as f:\n",
        "\n",
        "#infile = \"/content/dataset_easy-twitter-search-scraper_2023-11-21_01-10-14-456.json\" # This has Stock\n",
        "#infile = \"/content/dataset_easy-twitter-search-scraper_2023-11-21_18-49-26-401.json\"  # username filters\n",
        "#infile = '/content/dataset_easy-twitter-search-scraper_2023-11-22_01-36-29-112.json' # no username filter\n",
        "\n",
        "# Data pulled from https://console.apify.com/actors/2s3kSMq7tpuC3bI6M/runs/cMp5HramffKLAmVSt#output\n",
        "infile = f\"/content/{Tweets_filename}\"\n",
        "\n",
        "print(infile)\n",
        "with open(infile, \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "LM__sE5Z0D24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "f48d6ca0-309b-49b6-b74a-3c82aa43e505"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Tweets.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e4168a2af657>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Set up Username Counts\n",
        "'''\n",
        "\n",
        "username =[\"@USNewsMoney\", \"@TMFStockAdvisor\", \"@ftfinancenews\", \"@Stocktwits\", \"@MadMoneyOnCNBC\", \"@SquawkCNBC\"]\n",
        "\n",
        "cols = 2\n",
        "rows = len(username)\n",
        "username_counts = [[0 for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "# Fill the first column with values from the list\n",
        "for i in range(rows):\n",
        "    username_counts[i][0] = username[i]\n",
        "\n",
        "# Printing the 2D array\n",
        "#for row in username_counts:    print(row)\n"
      ],
      "metadata": {
        "id": "K3p57jJBdvCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Set up selectQ Counts\n",
        "'''\n",
        "searchQuery = [\"stock\",\"Apple\",\"Microsoft\",\"Alphabet\",\"Amazon\",\"Nvidia\",\"Tesla\",\"GOOG\",\"AMZN\",\"AAPL\",\"META\",\"MSFT\",\"NVDA\",\"TSLA\"]\n",
        "\n",
        "cols = 2\n",
        "rows = len(searchQuery)\n",
        "searchQuery_counts = [[0 for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "# Fill the first column with values from the list\n",
        "for i in range(rows):\n",
        "    searchQuery_counts[i][0] = searchQuery[i]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sd2MNjT4zDOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of Twitter Sentament"
      ],
      "metadata": {
        "id": "tGniHGe_E29l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point I want to add the code to do the sentiment alalisis.\n",
        "\n"
      ],
      "metadata": {
        "id": "n-fnSw-Im217"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "\n",
        "\n",
        "# load model and tokenizer\n",
        "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
        "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
        "\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n"
      ],
      "metadata": {
        "id": "75BicuOmm9rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocesses_tweet(tweet):\n",
        "  tweet_words = []\n",
        "\n",
        "  # replace\n",
        "  for word in tweet.split(' '):\n",
        "    # username\n",
        "    if word.startswith('@') and len(word) > 1 :\n",
        "      word = '@user'\n",
        "\n",
        "    # http link\n",
        "    elif word.startswith('http'):\n",
        "      word = \"http\"\n",
        "\n",
        "    elif word == '':\n",
        "      continue\n",
        "\n",
        "    tweet_words.append(word)\n",
        "\n",
        "  tweet_proc = \" \".join(tweet_words)\n",
        "  return tweet_proc\n"
      ],
      "metadata": {
        "id": "7dWCHf6CrFur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweet_sentiment(tweet_proc):\n",
        "  # sentiment analysis\n",
        "  encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')\n",
        "  # output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])\n",
        "  output = model(**encoded_tweet)\n",
        "\n",
        "  scores = output[0][0].detach().numpy()\n",
        "  scores = softmax(scores)\n",
        "\n",
        "  maxposition = 0\n",
        "  maxscore = 0\n",
        "\n",
        "  for i in range(len(scores)):\n",
        "    if maxscore <= scores[i]:\n",
        "      maxposition = i\n",
        "      maxscore = scores[i]\n",
        "\n",
        "  return labels[maxposition]  , scores[maxposition]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kofDI5AUtuy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_data = []\n",
        "loop_count = -1\n",
        "\n",
        "# Need to create a tweet loop\n",
        "\n",
        "for x in data:\n",
        "  loop_count+=1\n",
        "  try:\n",
        "    tweet_proc = preprocesses_tweet(x['text'])\n",
        "    sentiment_label, sentiment_score = get_tweet_sentiment(tweet_proc)\n",
        "  except Exception as e:\n",
        "    print(f\"exception on {str(loop_count)}\")\n",
        "    print(f\"Text:\",x['text'])\n",
        "    continue\n",
        "\n",
        "  out_data.append(\n",
        "        {\"timestamp\":x['timestamp']\n",
        "        ,\"username\":x['user']['username']\n",
        "        ,\"searchQuery\":x['searchQuery']\n",
        "        ,\"sentement\":sentiment_label\n",
        "        ,\"score\":str(sentiment_score)\n",
        "        ,\"text\":tweet_proc})\n",
        "\n"
      ],
      "metadata": {
        "id": "aWoAqEnpm-r0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547de248-2f3d-4a1b-8e6b-c1a9d1176e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exception on 66\n",
            "exception on 94\n",
            "exception on 222\n",
            "exception on 299\n",
            "exception on 412\n",
            "exception on 557\n",
            "exception on 625\n",
            "exception on 743\n",
            "exception on 856\n",
            "exception on 907\n",
            "exception on 948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_name = infile[:-5] + \"_out.csv\"\n",
        "\n",
        "# Open the CSV file in write mode\n",
        "with open(csv_file_name, mode='w', newline='') as csv_file:\n",
        "    # Create a CSV writer\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write the header row based on the keys in the JSON data\n",
        "    header = out_data[0].keys()\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write each row of data from the JSON object\n",
        "    for row in out_data:\n",
        "        csv_writer.writerow(row.values())"
      ],
      "metadata": {
        "id": "lohpJGAryXdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aquiring Stock Prices"
      ],
      "metadata": {
        "id": "hYhakd00FjVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Joining Stock Prices with respect to Tweets"
      ],
      "metadata": {
        "id": "xqKMuf0yFsqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analytics"
      ],
      "metadata": {
        "id": "zj2urhqMF0zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclution"
      ],
      "metadata": {
        "id": "9MeTB8d-F9xi"
      }
    }
  ]
}