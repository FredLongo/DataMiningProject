{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs+6MgLtQ03zBpwoOYy2Ej",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FredLongo/DataMiningProject/blob/main/code/GetTwitterSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first part is about processing the Twitter data to a format we can work with.   \n"
      ],
      "metadata": {
        "id": "-gt85Llw_0cp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "SBQdtyBwzosZ"
      },
      "outputs": [],
      "source": [
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#with open(\"twiterscraper.json\", \"r\") as f:\n",
        "#infile = \"/content/dataset_easy-twitter-search-scraper_2023-11-21_01-10-14-456.json\"\n",
        "infile = \"/content/dataset_easy-twitter-search-scraper_2023-11-21_18-49-26-401.json\"\n",
        "# Data pulled from https://console.apify.com/actors/2s3kSMq7tpuC3bI6M/runs/cMp5HramffKLAmVSt#output\n",
        "\n",
        "with open(infile, \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "LM__sE5Z0D24"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Set up Username Counts\n",
        "'''\n",
        "\n",
        "username =[\"@USNewsMoney\", \"@TMFStockAdvisor\", \"@ftfinancenews\", \"@Stocktwits\", \"@MadMoneyOnCNBC\", \"@SquawkCNBC\"]\n",
        "\n",
        "cols = 2\n",
        "rows = len(username)\n",
        "username_counts = [[0 for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "# Fill the first column with values from the list\n",
        "for i in range(rows):\n",
        "    username_counts[i][0] = username[i]\n",
        "\n",
        "# Printing the 2D array\n",
        "#for row in username_counts:    print(row)\n"
      ],
      "metadata": {
        "id": "K3p57jJBdvCL"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Set up selectQ Counts\n",
        "'''\n",
        "searchQuery = [\"stock\",\"Apple\",\"Microsoft\",\"Alphabet\",\"Amazon\",\"Nvidia\",\"Tesla\",\"GOOG\",\"AMZN\",\"AAPL\",\"META\",\"MSFT\",\"NVDA\",\"TSLA\"]\n",
        "\n",
        "cols = 2\n",
        "rows = len(searchQuery)\n",
        "searchQuery_counts = [[0 for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "# Fill the first column with values from the list\n",
        "for i in range(rows):\n",
        "    searchQuery_counts[i][0] = searchQuery[i]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sd2MNjT4zDOa"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "x_count = 0 #120\n",
        "user_count = 0   #2900\n",
        "\n",
        "for x in data:\n",
        "  x_count+=1\n",
        "\n",
        "  #get username_counts\n",
        "  username_counts[username.index(x['user']['username'].rstrip())][1]+=1\n",
        "  Qu = username_counts[username.index(x['user']['username'].rstrip())][1]\n",
        "\n",
        "  #get searchQuery_counts\n",
        "  searchQuery_counts[searchQuery.index(x['searchQuery'].rstrip())][1]+=1\n",
        "  Qv = searchQuery_counts[searchQuery.index(x['searchQuery'].rstrip())][1]\n",
        "\n",
        "  print(x_count, x['searchQuery'] ,'=', Qv,x['user']['username'],'=',Qu,'  text:',x['text'])\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "  Reviewing Data in file\n",
        "  print('>>>',x_count,'   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
        "  print('id:',x['id'])\n",
        "  print('url:',x['url'])\n",
        "  print('verified:',x['verified'])\n",
        "  print('timestamp:',x['timestamp'])\n",
        "  print('text:',x['text'])\n",
        "  print('links:',x['links'])\n",
        "  print('isQuote:',x['isQuote'])\n",
        "  print('isRetweet:',x['isRetweet'])\n",
        "  print('likes:',x['likes'])\n",
        "  print('replies:',x['replies'])\n",
        "  print('retweets:',x['retweets'])\n",
        "  print('quotes:',x['quotes'])\n",
        "  print('searchQuery:',x['searchQuery'])\n",
        "  print('user:',x['user']['username'])\n",
        "  print('\\n\\n')\n",
        "'''"
      ],
      "metadata": {
        "id": "dKH04Lpo61xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "for row in searchQuery_counts:\n",
        "    print(row)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "for row in username_counts:\n",
        "    print(row)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg9g26-Xns0L",
        "outputId": "c83f4c9e-424d-4c10-82bd-a5ecc3fc6bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['stock', 0]\n",
            "['Apple', 23]\n",
            "['Microsoft', 14]\n",
            "['Alphabet', 5]\n",
            "['Amazon', 0]\n",
            "['Nvidia', 16]\n",
            "['Tesla', 21]\n",
            "['GOOG', 3]\n",
            "['AMZN', 39]\n",
            "['AAPL', 29]\n",
            "['META', 39]\n",
            "['MSFT', 22]\n",
            "['NVDA', 39]\n",
            "['TSLA', 42]\n",
            "\n",
            "\n",
            "['@USNewsMoney', 4]\n",
            "['@TMFStockAdvisor', 0]\n",
            "['@ftfinancenews', 6]\n",
            "['@Stocktwits', 174]\n",
            "['@MadMoneyOnCNBC', 8]\n",
            "['@SquawkCNBC', 100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "out_data = []\n",
        "for x in data:\n",
        "  out_data.append(\n",
        "        {\"timestamp\":x['timestamp']\n",
        "        ,\"username\":x['user']['username']\n",
        "        ,\"searchQuery\":x['searchQuery']\n",
        "        ,\"sentement\":\"\"\n",
        "        ,\"score\":\"\"\n",
        "        ,\"text\":x['text']})\n",
        "'''\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gsWz1FDsiTSm"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point I want to add the code to do the sentiment alalisis.\n",
        "\n"
      ],
      "metadata": {
        "id": "n-fnSw-Im217"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "\n",
        "\n",
        "# load model and tokenizer\n",
        "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
        "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
        "\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "75BicuOmm9rm"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocesses_tweet(tweet):\n",
        "  tweet_words = []\n",
        "\n",
        "  # replace\n",
        "  for word in tweet.split(' '):\n",
        "    # username\n",
        "    if word.startswith('@') and len(word) > 1 :\n",
        "      word = '@user'\n",
        "\n",
        "    # http link\n",
        "    elif word.startswith('http'):\n",
        "      word = \"http\"\n",
        "\n",
        "    elif word == '':\n",
        "      continue\n",
        "\n",
        "    tweet_words.append(word)\n",
        "\n",
        "  tweet_proc = \" \".join(tweet_words)\n",
        "  return tweet_proc\n"
      ],
      "metadata": {
        "id": "7dWCHf6CrFur"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweet_sentiment(tweet_proc):\n",
        "  # sentiment analysis\n",
        "  encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')\n",
        "  # output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])\n",
        "  output = model(**encoded_tweet)\n",
        "\n",
        "  scores = output[0][0].detach().numpy()\n",
        "  scores = softmax(scores)\n",
        "\n",
        "  maxposition = 0\n",
        "  maxscore = 0\n",
        "\n",
        "  for i in range(len(scores)):\n",
        "    if maxscore <= scores[i]:\n",
        "      maxposition = i\n",
        "      maxscore = scores[i]\n",
        "\n",
        "  return labels[maxposition]  , scores[maxposition]\n",
        "\n",
        "#  for i in range(len(scores)):\n",
        "#    l = labels[i]\n",
        "#    s = scores[i]\n",
        "#    print(l,s)\n",
        "\n"
      ],
      "metadata": {
        "id": "kofDI5AUtuy7"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_data = []\n",
        "\n",
        "# Need to create a tweet loop\n",
        "\n",
        "for x in data:\n",
        "  tweet_proc = preprocesses_tweet(x['text'])\n",
        "  sentiment_label, sentiment_score = get_tweet_sentiment(tweet_proc)\n",
        "  out_data.append(\n",
        "        {\"timestamp\":x['timestamp']\n",
        "        ,\"username\":x['user']['username']\n",
        "        ,\"searchQuery\":x['searchQuery']\n",
        "        ,\"sentement\":sentiment_label\n",
        "        ,\"score\":str(sentiment_score)\n",
        "        ,\"text\":tweet_proc})\n",
        "\n",
        "\n",
        "  #print(x['timestamp'], x['user']['username'],x['searchQuery'],sentiment_label, sentiment_score, tweet_proc)\n",
        "\n",
        "\n",
        "    #break  # Exit the loop... for testing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aWoAqEnpm-r0"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outfilename = infile[:-5] + \"_out.json\"\n",
        "\n",
        "with open(outfilename, \"w\") as file:\n",
        "    json.dump(out_data, file)"
      ],
      "metadata": {
        "id": "ZeCJ1dohL5VI"
      },
      "execution_count": 98,
      "outputs": []
    }
  ]
}