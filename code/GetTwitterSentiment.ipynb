{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO63iAQ3uzdUuWwak39L4uS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FredLongo/DataMiningProject/blob/main/code/GetTwitterSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first part is about processing the Twitter data to a format we can work with.   \n"
      ],
      "metadata": {
        "id": "-gt85Llw_0cp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SBQdtyBwzosZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "#This pulls in the Tweets data\n",
        "#Tweets URL of the file to be downloaded\n",
        "\n",
        "Tweets_url = 'https://drive.google.com/file/d/1s429__b-2Gptf2oN2g-t--1aNPkMiyU-/view?usp=drive_link'\n",
        "\n",
        "Tweets_filename = 'Tweets.json'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "Tweets_response = requests.get(Tweets_url)\n",
        "\n",
        "# Ensure the request was successful\n",
        "if Tweets_response.status_code == 200:\n",
        "    # Write the content of the response to a file\n",
        "    with open(Tweets_filename, 'wb') as file:\n",
        "        file.write(Tweets_response.content)\n",
        "else:\n",
        "    print(f\"Failed to download the file. Status code: {Tweets_response.status_code}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7dVV8pfAKiki"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Stock Price URL of the file to be downloaded\n",
        "StockPrice_url = 'https://drive.google.com/file/d/1Hk3Ca66ai_vAO14EFAD1AfOEk2A60E-U/view?usp=drive_link'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "StockPrice_response = requests.get(StockPrice_url)\n",
        "\n",
        "# Ensure the request was successful\n",
        "if StockPrice_response.status_code == 200:\n",
        "    # Write the content of the response to a file\n",
        "    with open('StockPrice.csv', 'wb') as file:\n",
        "        file.write(StockPrice_response.content)\n",
        "else:\n",
        "    print(f\"Failed to download the file. Status code: {StockPrice_response.status_code}\")\n"
      ],
      "metadata": {
        "id": "kUpZkFsVrO7h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#with open(\"twiterscraper.json\", \"r\") as f:\n",
        "\n",
        "#infile = \"/content/dataset_easy-twitter-search-scraper_2023-11-21_01-10-14-456.json\" # This has Stock\n",
        "#infile = \"/content/dataset_easy-twitter-search-scraper_2023-11-21_18-49-26-401.json\"  # username filters\n",
        "#infile = '/content/dataset_easy-twitter-search-scraper_2023-11-22_01-36-29-112.json' # no username filter\n",
        "\n",
        "# Data pulled from https://console.apify.com/actors/2s3kSMq7tpuC3bI6M/runs/cMp5HramffKLAmVSt#output\n",
        "infile = f\"/content/{Tweets_filename}\"\n",
        "\n",
        "print(infile)\n",
        "with open(infile, \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "LM__sE5Z0D24",
        "outputId": "f48d6ca0-309b-49b6-b74a-3c82aa43e505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Tweets.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e4168a2af657>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Set up Username Counts\n",
        "'''\n",
        "\n",
        "username =[\"@USNewsMoney\", \"@TMFStockAdvisor\", \"@ftfinancenews\", \"@Stocktwits\", \"@MadMoneyOnCNBC\", \"@SquawkCNBC\"]\n",
        "\n",
        "cols = 2\n",
        "rows = len(username)\n",
        "username_counts = [[0 for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "# Fill the first column with values from the list\n",
        "for i in range(rows):\n",
        "    username_counts[i][0] = username[i]\n",
        "\n",
        "# Printing the 2D array\n",
        "#for row in username_counts:    print(row)\n"
      ],
      "metadata": {
        "id": "K3p57jJBdvCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Set up selectQ Counts\n",
        "'''\n",
        "searchQuery = [\"stock\",\"Apple\",\"Microsoft\",\"Alphabet\",\"Amazon\",\"Nvidia\",\"Tesla\",\"GOOG\",\"AMZN\",\"AAPL\",\"META\",\"MSFT\",\"NVDA\",\"TSLA\"]\n",
        "\n",
        "cols = 2\n",
        "rows = len(searchQuery)\n",
        "searchQuery_counts = [[0 for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "# Fill the first column with values from the list\n",
        "for i in range(rows):\n",
        "    searchQuery_counts[i][0] = searchQuery[i]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sd2MNjT4zDOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point I want to add the code to do the sentiment alalisis.\n",
        "\n"
      ],
      "metadata": {
        "id": "n-fnSw-Im217"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "\n",
        "\n",
        "# load model and tokenizer\n",
        "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
        "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
        "\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n"
      ],
      "metadata": {
        "id": "75BicuOmm9rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocesses_tweet(tweet):\n",
        "  tweet_words = []\n",
        "\n",
        "  # replace\n",
        "  for word in tweet.split(' '):\n",
        "    # username\n",
        "    if word.startswith('@') and len(word) > 1 :\n",
        "      word = '@user'\n",
        "\n",
        "    # http link\n",
        "    elif word.startswith('http'):\n",
        "      word = \"http\"\n",
        "\n",
        "    elif word == '':\n",
        "      continue\n",
        "\n",
        "    tweet_words.append(word)\n",
        "\n",
        "  tweet_proc = \" \".join(tweet_words)\n",
        "  return tweet_proc\n"
      ],
      "metadata": {
        "id": "7dWCHf6CrFur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweet_sentiment(tweet_proc):\n",
        "  # sentiment analysis\n",
        "  encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')\n",
        "  # output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])\n",
        "  output = model(**encoded_tweet)\n",
        "\n",
        "  scores = output[0][0].detach().numpy()\n",
        "  scores = softmax(scores)\n",
        "\n",
        "  maxposition = 0\n",
        "  maxscore = 0\n",
        "\n",
        "  for i in range(len(scores)):\n",
        "    if maxscore <= scores[i]:\n",
        "      maxposition = i\n",
        "      maxscore = scores[i]\n",
        "\n",
        "  return labels[maxposition]  , scores[maxposition]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kofDI5AUtuy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_data = []\n",
        "loop_count = -1\n",
        "\n",
        "# Need to create a tweet loop\n",
        "\n",
        "for x in data:\n",
        "  loop_count+=1\n",
        "  try:\n",
        "    tweet_proc = preprocesses_tweet(x['text'])\n",
        "    sentiment_label, sentiment_score = get_tweet_sentiment(tweet_proc)\n",
        "  except Exception as e:\n",
        "    print(f\"exception on {str(loop_count)}\")\n",
        "    print(f\"Text:\",x['text'])\n",
        "    continue\n",
        "\n",
        "  out_data.append(\n",
        "        {\"timestamp\":x['timestamp']\n",
        "        ,\"username\":x['user']['username']\n",
        "        ,\"searchQuery\":x['searchQuery']\n",
        "        ,\"sentement\":sentiment_label\n",
        "        ,\"score\":str(sentiment_score)\n",
        "        ,\"text\":tweet_proc})\n",
        "\n"
      ],
      "metadata": {
        "id": "aWoAqEnpm-r0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547de248-2f3d-4a1b-8e6b-c1a9d1176e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exception on 66\n",
            "exception on 94\n",
            "exception on 222\n",
            "exception on 299\n",
            "exception on 412\n",
            "exception on 557\n",
            "exception on 625\n",
            "exception on 743\n",
            "exception on 856\n",
            "exception on 907\n",
            "exception on 948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_name = infile[:-5] + \"_out.csv\"\n",
        "\n",
        "# Open the CSV file in write mode\n",
        "with open(csv_file_name, mode='w', newline='') as csv_file:\n",
        "    # Create a CSV writer\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write the header row based on the keys in the JSON data\n",
        "    header = out_data[0].keys()\n",
        "    csv_writer.writerow(header)\n",
        "\n",
        "    # Write each row of data from the JSON object\n",
        "    for row in out_data:\n",
        "        csv_writer.writerow(row.values())"
      ],
      "metadata": {
        "id": "lohpJGAryXdK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}